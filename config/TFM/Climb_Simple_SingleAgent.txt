* Test 1
       behaviors:
        Climb:
          trainer_type: ppo
          hyperparameters:
            batch_size: 256
            buffer_size: 4096
            learning_rate: 0.0003
            beta: 0.01
            epsilon: 0.2
            lambd: 0.95
            num_epoch: 8
            learning_rate_schedule: linear
          network_settings:
            normalize: true
            hidden_units: 512
            num_layers: 2
            vis_encode_type: simple
          reward_signals:
            extrinsic:
              gamma: 0.99
              strength: 1.0  
            curiosity:
              gamma: 0.99
              strength: 0.05
              gamma: 0.99
              network_settings:
                hidden_units: 256
              learning_rate: 0.0003
          keep_checkpoints: 50
          max_steps: 2000001
          time_horizon: 128
          summary_freq: 30000
    
        Rewards:
            Boton +1
            Goal: +4
            Fail: -1
            
        Observaciones:            
            Target (x,y,z)
            Agent  (x,y,z)
            Agent Vel (x)
            Agent Vel (z)

        9 rays para no caerse: suelo, switchon, bridge
        12 rays para el perimetro: paredes, meta, boton
        camara de 21x21 (20x20x minimo)

    Configuracion entorno: 
        public bool UseRandomAgentPosition = true;
        public bool UseRandomAgentRotation = true;
        public bool UseRandomBumps = false

    Rewards:
        -1 negative:    - failed episode: -0.5
                        - -1/maxsteps si salta
        +1 positive:    - Goal: 0.5
                        - checkpoints: 0.5

    
test 2:
   
    añadir float para modificar la random_pos de los objetos
    añadir curriculum learning:
        - name: Lesson0 # This is the start of the second lesson
        completion_criteria:
          measure: reward
          behavior: Bridge
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 0.5
        value: 0.0
      - name: Lesson1
        completion_criteria:
          measure: reward
          behavior: Bridge
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 0.6
        value:
          sampler_type: uniform
          sampler_parameters:
            min_value: 0.0
            max_value: 0.2
      - name: Lesson2
        completion_criteria:
          measure: reward
          behavior: Bridge
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 0.7
        value:
          sampler_type: uniform
          sampler_parameters:
            min_value: 0.2
            max_value: 0.5
      - name: Lesson3
        completion_criteria:
          measure: reward
          behavior: Bridge
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 0.8
        value:
          sampler_type: uniform
          sampler_parameters:
            min_value: 0.5
            max_value: 0.8
      - name: Lesson4
        completion_criteria:
          measure: reward
          behavior: Bridge
          signal_smoothing: true
          min_lesson_length: 100
          threshold: 0.85
        value:
          sampler_type: uniform
          sampler_parameters:
            min_value: 0.8
            max_value: 1.0
      - name: Lesson5
        value: 1.0

      
     quitar recompensa salto
     a;adir recompensa negativa cada step     
     no parece que aprenda mucho
     a;adir checkpoints al raycast
     a;adir observacion con pos xyz del siguiente checkpoint
     a;adir observacion checkpoint al raycast superior
   
test 2:
    quitar salto
    reward negativa al tocar pared y checkpoint incorrecto
    
    track checkpoints parece que no va bien, no contaba bien el siguiente checkpoint
    ya funciona bien, arreglar alguna cuesta que se queda pillado
    
    //alejar goal poco a poco74

test 3:
    volver a meter el salto y los obstaculos
    normalizar rewards -1,1
    quitar obs target (-3 vector space size -> 8)
    

